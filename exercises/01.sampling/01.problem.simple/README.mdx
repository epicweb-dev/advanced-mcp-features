# Simple Sampling

üë®‚Äçüíº Our users love the prompt functionality, but they asked why the LLM couldn't
just suggest tags when they create the post and we thought that's a great idea!

So now your goal is to make your server request a simple completion from the
language model whenever a new journal entry is created.

In this first step, we'll just get things wired up and then we'll work on our
prompt for the LLM in the next step.

**Here's what you'll do:**

- Implement a function that sends a sampling request to the client using
  `agent.server.server.createMessage` (the `server.server` thing is funny, but
  our MCP server manages an internal server and that's what we're accessing).
- Use a simple system prompt (e.g., "You are a helpful assistant.") and a user
  message that references the new journal entry's ID (we'll enhance this next).
- Set a reasonable `maxTokens` value for the response.
- Parse the model's response using a provided Zod schema.
- Log the result to the console so you can inspect the model's output.

And don't forget to call it when the user creates a new journal entry!

The `maxTokens` option references the max tokens the model should return.

<callout-warning>
	The `system prompt` + `messages` + `OUTPUT MESSAGE` can't exceed the context
	window size of the model you're user is using.
</callout-warning>

This step will help you get comfortable with the basic request/response flow for
sampling in MCP, and set the stage for more advanced prompt engineering in the
next step.
